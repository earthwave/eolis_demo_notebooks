{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osWavvIYpHpT"
      },
      "source": [
        "# Assessing Subglacial Lakes Dynamics in the Thwaites area, Western Antarctica with CryoSat-2 radar altimetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkVKaJMppZPB"
      },
      "source": [
        "CryoSat-2 is a European Space Agency (ESA) satellite launched in 2010. It's main job is to measure changes in the thickness and volume of ice - over land and sea - to help us understand the cryosphere's role in climate change.\n",
        "\n",
        "In this tutorial, we will use the **CryoTEMPO Land Ice** and **CryoTEMPO-EOLIS** gridded products to study the evolution of the surface elevation of four subglacial lakes underneath Thwaites glacier, in Western Antarctica.\n",
        "These are altimetry datasets derived from CryoSat-2 measurements using two distinct processing techniques: **Point of Closest Approach** (POCA), and **Swath**.\n",
        "\n",
        "Along the way you will find some questions. Please provide the answers using the questionnaire at [this link](https://forms.gle/aYiLViyEvqmJfe1N7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4PirYISzqRw"
      },
      "source": [
        "### Environment Setup\n",
        "To run this notebook, you will need to make sure that the following packages are installed in your python environment (all can be installed via pip/conda):\n",
        "- matplotlib\n",
        "- geopandas\n",
        "- contextily\n",
        "- ipykernel\n",
        "- shapely\n",
        "- specklia\n",
        "\n",
        "If you are using the Google Colab environment, these packages will be installed in the next cell. Please note this step may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT0m7yFmnjPk",
        "outputId": "4193b7f1-0df0-4303-f3a3-5b0038372e64"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    %pip install rasterio --no-binary rasterio\n",
        "    %pip install specklia\n",
        "    %pip install matplotlib\n",
        "    %pip install geopandas\n",
        "    %pip install contextily\n",
        "    %pip install shapely\n",
        "    %pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaYLPUORnNwT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from time import perf_counter\n",
        "from datetime import datetime, timedelta\n",
        "import geopandas as gpd\n",
        "import math\n",
        "import pprint\n",
        "import pandas as pd\n",
        "from shapely import box, Point, Polygon\n",
        "\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "\n",
        "import contextily as ctx\n",
        "from specklia import Specklia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIucoOIoTk5H"
      },
      "source": [
        "# Radar altimetry point data\n",
        "\n",
        "Satellite altimetry measurements are often provided in point data format. The satellite measures the surface elevation of the ground along its orbit by sending an electromagnetic signal and receiving it back, whether it is laser (e.g. ICESat-2) or radar (e.g. CryoSat-2).\n",
        "\n",
        "Let's get some point data measured by CryoSat-2. The dataset we are going to use now is called **CryoTEMPO Land Ice Thematic Product**. This dataset can be queried via [Specklia](https://specklia.earthwave.co.uk/), a database that provides access to various altimetry datasets.\n",
        "\n",
        "Specklia is free to use, but an API key needs to be generated and passed to the next cell in order to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAePMv3bU0pZ",
        "outputId": "94a0b6b4-9003-4872-fd6b-c2165e44cc71"
      },
      "outputs": [],
      "source": [
        "user_api_key = input('Please generate your own key using https://specklia.earthwave.co.uk/ApiKeys and paste it here: ')\n",
        "client = Specklia(user_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B75XkbLYWfK"
      },
      "source": [
        "## Define an area of interest and the time interval for the initial query\n",
        "\n",
        "To get started, we'll be looking at all the measurements taken during one month (January 2013) within a rectangular area of size 3° x 2° centred on longitude 107°W and latitude 76.5°S (in Western Antarctica).\n",
        "\n",
        "📌 Note: These coordinates use a global reference system called **WGS84**, identified by the code **EPSG:4326**.\n",
        "\n",
        "Later in this notebook, we’ll switch to a coordinate system made especially for Antarctica, called **Antarctic Polar Stereographic (EPSG:3031)**, which makes maps and calculations in the polar region easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYGPkTFJYQSN"
      },
      "outputs": [],
      "source": [
        "query_polygon_4326 = Polygon((\n",
        "    (-108.5, -77.5), (-105.5, -77.5), (-105.5, -75.5), (-108.5, -75.5), (-108.5, -77.5)))\n",
        "query_extent_4326 = gpd.GeoSeries(query_polygon_4326, crs=4326)\n",
        "\n",
        "query_start_time = datetime(2013, 1, 1)\n",
        "query_end_time = datetime(2013, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSbXUcYrdpXU"
      },
      "source": [
        "Let’s take a look at where we are in Antarctica!\n",
        "\n",
        "In the figure below:\n",
        "- The red rectangle shows our area of interest\n",
        "- The background is a satellite image of West Antarctica’s coastline - giving us a clear view of the icy landscape we’re studying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "jq2FORKCcfdb",
        "outputId": "e970b975-a324-4464-91af-398cdbbb28d8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "ax.plot(*query_polygon_4326.exterior.xy, c='r', lw=3)\n",
        "ax.set_xlim(left=-150, right=0)\n",
        "ax.set_ylim(bottom=-85, top=-55)\n",
        "\n",
        "ax.set_ylabel('latitude  [°]', fontsize=20)\n",
        "ax.set_xlabel('longitude  [°]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=3, crs=4326, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGOyISJZwc4"
      },
      "source": [
        "## First query of point data: POCA points\n",
        "\n",
        "The measurements included in the CryoTEMPO Land Ice Thematic Product are processed with the Point Of Closest Approach (POCA) algorithm. For this reason, we'll indicate the data obtained with this first query with the prefix `poca`.\n",
        "\n",
        "\n",
        "Before querying the actual data, we can initially check what information about the dataset is available. For example we can look at the time span of the entire dataset and the columns it contains.\n",
        "It can be very useful to know in advance what each column represents, the units and the range of values associated with each of them etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnYGBoqebklH",
        "outputId": "2f58aaeb-35d6-4d28-f7fb-da029e0810a4"
      },
      "outputs": [],
      "source": [
        "# this is the name of the dataset we are going to use\n",
        "dataset_name = 'CryoTEMPO Land Ice Thematic Product Baseline C'\n",
        "\n",
        "available_datasets = client.list_datasets()\n",
        "poca_point_dataset = available_datasets[\n",
        "    available_datasets['dataset_name'] == dataset_name].iloc[0]\n",
        "\n",
        "\n",
        "print(f\"{dataset_name} contains data between \\n\"\n",
        "      f\"{poca_point_dataset['min_timestamp']} \"\n",
        "      f\"and {poca_point_dataset['max_timestamp']}\\n\")\n",
        "\n",
        "print(f\"{dataset_name} has the following columns:\")\n",
        "pprint.PrettyPrinter(indent=2, width=120).pprint(poca_point_dataset['columns'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzLNzyfcrHn"
      },
      "source": [
        "Now lets execute the query amd preview the results! It shouldn't take more than a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Jn1jP_esTkM9",
        "outputId": "2ea49f31-9cad-4902-845f-5e6f6f2200d8"
      },
      "outputs": [],
      "source": [
        "poca_point_data, poca_point_data_sources = client.query_dataset(\n",
        "    dataset_id=poca_point_dataset['dataset_id'],\n",
        "    epsg4326_polygon=query_polygon_4326,\n",
        "    min_datetime=query_start_time,\n",
        "    max_datetime=query_end_time)\n",
        "\n",
        "print(f'This query returned {len(poca_point_data)} points, drawn from {len(poca_point_data_sources)} original sources.')\n",
        "\n",
        "poca_point_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4KIri3sdUlV"
      },
      "source": [
        "### Understanding the POCA dataset columns\n",
        "\n",
        "We have queried all the columns available in the database. Here’s a quick overview of what each one means:\n",
        "\n",
        "- `elevation` is the surface elevation measurement\n",
        "\n",
        "- `uncertainty` is the estimated uncertainty associated with the elevation measurement.\n",
        "\n",
        "- `timestamp` indicates the time when each observation was made, in units of seconds since a standard time (normally, the 1st January 1970 at midnight).\n",
        "\n",
        "- `geometry` contains the spatial coordinates (longitude and latitude), stored as a shapely.Point object.\n",
        "\n",
        "- `reference_dem` indicates the surface elevation reported by a Digital Elevation Model at the same location of the measurement. Often we calculate the difference between the measured elevation and the reference DEM, hence removing the topography and allowing to compare elevation changes at different locations.\n",
        "\n",
        "Now, let’s visualise the data!\n",
        "The figure in the next cell shows each measurement as a dot located using geometry, and colored based on its elevation value 🏔️. This gives us a first glance at the spatial patterns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "dx1j8hmChCMH",
        "outputId": "4d5bf41b-639d-4de5-a68c-a9ead1da3a13"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='7%', pad=0.15)\n",
        "scatter_plot = ax.scatter(\n",
        "    x=poca_point_data.geometry.x,\n",
        "    y=poca_point_data.geometry.y,\n",
        "    c=poca_point_data.elevation,\n",
        "    s=2)\n",
        "ax.set_xlim(left=query_polygon_4326.bounds[0], right=query_polygon_4326.bounds[2])\n",
        "ax.set_ylim(bottom=query_polygon_4326.bounds[1], top=query_polygon_4326.bounds[3])\n",
        "cbar = fig.colorbar(scatter_plot, cax=cax, orientation='vertical')\n",
        "cbar.set_label('Elevation  [m]', rotation=-90, fontsize=16, labelpad=30)\n",
        "ax.set_ylabel('latitude  [°]', fontsize=20)\n",
        "ax.set_xlabel('longitude  [°]', fontsize=20)\n",
        "ax.set_aspect('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrByzcJJRqOW"
      },
      "source": [
        "We can also add a little map to understand where the area of interest is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "UNAMVLcSN7Qj",
        "outputId": "ce16f192-c31a-4905-992c-ae2992fbaab8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 12))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "ax.plot(*query_polygon_4326.exterior.xy, c='r', lw=3)\n",
        "scatter_plot = ax.scatter(\n",
        "    x=poca_point_data.geometry.x,\n",
        "    y=poca_point_data.geometry.y,\n",
        "    c=poca_point_data.elevation,\n",
        "    s=2)\n",
        "ax.set_xlim(left=query_polygon_4326.bounds[0] - 1.5, right=query_polygon_4326.bounds[2] + 1.5)\n",
        "ax.set_ylim(bottom=query_polygon_4326.bounds[1] - 1.5, top=query_polygon_4326.bounds[3] + 1.5)\n",
        "\n",
        "cbar = fig.colorbar(scatter_plot, cax=cax, orientation='vertical')\n",
        "cbar.set_label('Elevation  [m]', rotation=-90, fontsize=20, labelpad=30)\n",
        "ax.set_ylabel('latitude  [°]', fontsize=20)\n",
        "ax.set_xlabel('longitude  [°]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=4326, attribution=False)\n",
        "\n",
        "thumbnail_ax = inset_axes(ax, width='35%', height='15%', loc='upper left')\n",
        "thumbnail_ax.set_ylim(-80, -60)\n",
        "thumbnail_ax.set_xlim(-125, -45)\n",
        "thumbnail_ax.tick_params(bottom=False, left=False, labelbottom=True, labelleft=False, labelright=True)\n",
        "for spine in ['bottom', 'top', 'right', 'left']:\n",
        "    thumbnail_ax.spines[spine].set_color('red')\n",
        "thumbnail_ax.plot(*query_polygon_4326.exterior.xy, c='r', lw=3)\n",
        "ctx.add_basemap(thumbnail_ax, source=ctx.providers.Esri.WorldImagery, zoom=4, crs=4326, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0i75fhin1k"
      },
      "source": [
        "## Query of point data: Swath points\n",
        "\n",
        "Let's now query the **CryoTEMPO-EOLIS Point Product**, that also contains elevation point measurements, but generated with the [swath processing method](https://earth.esa.int/eogateway/news/cryosats-swath-processing-technique).\n",
        "\n",
        "More information can be found in the EOLIS Product Handbook at [this link](https://cryotempo-eolis.org/product-overview/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this is the name of the dataset we are going to use\n",
        "dataset_name = 'CryoTEMPO-EOLIS Point Product'\n",
        "\n",
        "swath_point_dataset = available_datasets[\n",
        "    available_datasets['dataset_name'] == dataset_name].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5IKMHDdRn4"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 1**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**📝 Before You Run the Next Cell: A Quick Question!**\n",
        "\n",
        "Before we run a query on our dataset, let’s pause for a moment and ask an important question:\n",
        "\n",
        "- *What kind of data does this dataset actually contain?*\n",
        "\n",
        "Knowing what variables are available - and the time period they cover - is *really* helpful. It helps you decide what kind of query you want to run and how to set it up.\n",
        "\n",
        "To figure this out, we can check the **metadata** for the dataset. You should have already retrieved it using `.list_datasets()` in the previous cells. The metadata gives you a summary of what’s inside the dataset.\n",
        "\n",
        "> **Question**: Look at the metadata for the **CryoTEMPO-EOLIS Point Product** (called `swath_point_dataset`).  \n",
        "> - What columns (variables) does this dataset include?\n",
        "> - What is the maximum timestep covered by the dataset?\n",
        "\n",
        "Take a look at the output in the cell above to find the answers.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "\n",
        "You can access parts of the metadata like this:\n",
        "\n",
        "- Use `swath_point_dataset['Columns']` to list the available columns (variables)\n",
        "- Use `swath_point_dataset['max_timestep']` to find the maximum timestep covered by the dataset\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5glE7cbJjci8",
        "outputId": "5d213d28-9105-4cb3-e213-fe672dba701b"
      },
      "outputs": [],
      "source": [
        "swath_point_data, swath_point_data_sources = client.query_dataset(\n",
        "    dataset_id=swath_point_dataset['dataset_id'],\n",
        "    epsg4326_polygon=query_polygon_4326,\n",
        "    min_datetime=query_start_time,\n",
        "    max_datetime=query_end_time)\n",
        "print(f'This query returned {len(swath_point_data)} points, drawn from {len(swath_point_data_sources)} original sources.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg8btobfj5Lz"
      },
      "source": [
        "Note the remarkable difference in the number of points retrieved - 5000 POCA points vs. more than one million Swath points, over the same area and within the same time window.\n",
        "\n",
        "Most of the columns are similar to the POCA dataset. `x` and `y` here indicate the projected spatial coordinates, in **Antarctic Polar Stereographic (EPSG: 3031)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Z_LiwX4PkKii",
        "outputId": "5a5a2fcc-7d4b-4517-a10b-96d85ce4e6ff"
      },
      "outputs": [],
      "source": [
        "swath_point_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsMnxPGTjkhD"
      },
      "source": [
        "Let's plot the POCA data and the Swath data next to each other.\n",
        "\n",
        "📌 Note: This may take up to 20 seconds due to the large volume of Swath data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ZWt62nZAiyxv",
        "outputId": "dd39a563-9232-413b-c9b9-d7512cc9bfba"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, sharex=True, figsize=(20, 8))\n",
        "ax = axes[0]\n",
        "poca_scatter_plot = ax.scatter(\n",
        "    x=poca_point_data.geometry.x,\n",
        "    y=poca_point_data.geometry.y,\n",
        "    c=poca_point_data.elevation,\n",
        "    s=2)\n",
        "ax.set_xlim(left=query_polygon_4326.bounds[0], right=query_polygon_4326.bounds[2])\n",
        "ax.set_ylim(bottom=query_polygon_4326.bounds[1], top=query_polygon_4326.bounds[3])\n",
        "ax.set_xlabel('longitude  [°]', fontsize=20)\n",
        "ax.set_ylabel('latitude  [°]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('POCA', fontsize=30)\n",
        "\n",
        "ax = axes[1]\n",
        "swath_scatter_plot = ax.scatter(\n",
        "    x=swath_point_data.geometry.x,\n",
        "    y=swath_point_data.geometry.y,\n",
        "    c=swath_point_data.elevation,\n",
        "    s=2)\n",
        "ax.set_xlim(left=query_polygon_4326.bounds[0], right=query_polygon_4326.bounds[2])\n",
        "ax.set_ylim(bottom=query_polygon_4326.bounds[1], top=query_polygon_4326.bounds[3])\n",
        "ax.set_xlabel('longitude  [°]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_title('Swath', fontsize=30)\n",
        "\n",
        "fig.subplots_adjust(right=0.89, hspace=0.05, wspace=0.08)\n",
        "cax = fig.add_axes([0.92, 0.15, 0.025, 0.7])\n",
        "cbar = fig.colorbar(swath_scatter_plot, cax=cax, orientation='vertical')\n",
        "cbar.set_label('Elevation  [m]', rotation=-90, fontsize=16, labelpad=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APx_acfifBfX"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 2**\n",
        "\n",
        "---\n",
        "\n",
        "**🔍 Quick Check: Comparing Swath and POCA Points**\n",
        "\n",
        "Now that you've run the Swath and POCA queries, let’s do a quick comparison:\n",
        "\n",
        "> **Question:** What is the ratio between the number of Swath points and the number of POCA points?\n",
        "\n",
        "This will help you understand how much data each product type gives you, and why you might choose one over the other in different situations.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "\n",
        "Look at the print statements that appeared after the query finished running. They tell you how many points were retrieved for each product.\n",
        "\n",
        "Try calculating the ratio:  `number of Swath points ÷ number of POCA points`\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N48F-1Crl1rv"
      },
      "source": [
        "# Gridding Radar Altimetry Data\n",
        "\n",
        "So far, we’ve seen that CryoSat-2 gives us point measurements, but those points are:\n",
        "\n",
        "- Not evenly spaced\n",
        "- Change position with each satellite pass\n",
        "\n",
        "This irregular spacing can make the data tricky to work with directly.\n",
        "\n",
        "To make it easier to extract information from the data, we often aggregate the points onto a grid (a process we refer to as **gridding**). Gridding helps simplify analysis and allows for consistent comparisons across time and space.\n",
        "\n",
        "In the rest of the notebook, instead of working with longitude and latitude coordinates, we will reproject the data in **Antarctic Polar Stereographic coordinates (EPSG: 3031)**.\n",
        "\n",
        "The function defined in the cell below contains a very simple method to do this.\n",
        "- First of all, we choose a resolution for the grid. The resolution is the same in both x and y spatial coordinates, and is in units of meters for projected coordinates.\n",
        "- Then we use a method, implemented in the python library pandas, to quickly join the points that belong to the same grid cell.\n",
        "- Finally, we calculate the median elevation in each grid cell, considering all the points that belong to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COcZ5yC_2aZ_"
      },
      "outputs": [],
      "source": [
        "def grid_data(point_data_gdf: gpd.GeoDataFrame, column_name: str, grid_resolution: float) -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate the points on a regular grid and calculate the median value of the column of interest within each grid cell.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    point_data_gdf : gpd.GeoDataFrame\n",
        "        The point data.\n",
        "    column_name : str\n",
        "        Column name of point data to calculate the median over.\n",
        "    grid_resolution : float\n",
        "        Target resolution of the regular grid. The resolution is the same in both spatial coordinates.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    gpd.GeoDataFrame\n",
        "        The gridded dataset containing the median value of the column within each grid cell.\n",
        "    \"\"\"\n",
        "    half_grid_res = grid_resolution / 2\n",
        "\n",
        "    # assign to each point the coordinates of the grid cell it belongs to\n",
        "    point_data_gdf['x_centre'] = [math.floor(x / grid_resolution) * grid_resolution + half_grid_res\n",
        "                                  for x in point_data_gdf.geometry.x]\n",
        "    point_data_gdf['y_centre'] = [math.floor(y / grid_resolution) * grid_resolution + half_grid_res\n",
        "                                  for y in point_data_gdf.geometry.y]\n",
        "\n",
        "    # use the pandas.DataFrame method \"groupby\" to aggregate the points that belong to the same grid cell\n",
        "    grid_cell_df = point_data_gdf.groupby(by=['x_centre', 'y_centre'])[column_name].agg('median').reset_index()\n",
        "\n",
        "    # construct another DataFrame where for each grid cell we retain the mean elevation\n",
        "    gridded_data_gdf = gpd.GeoDataFrame({\n",
        "        f'median_{column_name}': grid_cell_df[column_name],\n",
        "        'geometry': box(grid_cell_df.x_centre - half_grid_res,\n",
        "                        grid_cell_df.y_centre - half_grid_res,\n",
        "                        grid_cell_df.x_centre + half_grid_res,\n",
        "                        grid_cell_df.y_centre + half_grid_res)})\n",
        "\n",
        "    return gridded_data_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aREiwvPQWQ2j"
      },
      "source": [
        "Let's use the function above to derive a gridded dataset over the area of interest!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "DQNVTQ0NuUe_",
        "outputId": "32db3e91-f6bf-4c95-a4f7-0ea761d4171c"
      },
      "outputs": [],
      "source": [
        "# define the EPSG code of the Antarctic Polar Stereographic Projection\n",
        "aps_epsg_code = 3031\n",
        "# grid the data on a regular grid with 1km resolution\n",
        "grid_resolution_m = 1000\n",
        "# first reproject the point data to Antarctic Polar Stereographic\n",
        "swath_point_data_3031 = swath_point_data.to_crs(aps_epsg_code)\n",
        "# run the gridding\n",
        "gridded_data_3031 = grid_data(swath_point_data_3031.copy(), 'elevation', grid_resolution_m)\n",
        "\n",
        "# also reproject the area of interest\n",
        "query_polygon_3031 = query_extent_4326.to_crs(aps_epsg_code).iloc[0]\n",
        "\n",
        "# plot the gridded data and the large scale map in the inset\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "gridded_data_3031.plot(\n",
        "    'median_elevation', ax=ax,\n",
        "    cmap='viridis', cax=cax, legend=True,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "fig.get_axes()[1].set_ylabel('Median Elevation in Grid Cell [m]', rotation=-90, fontsize=20, labelpad=30)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=aps_epsg_code, attribution=False)\n",
        "\n",
        "thumbnail_ax = inset_axes(ax, width='30%', height='20%', loc='upper left')\n",
        "thumbnail_ax.tick_params(bottom=False, left=False, labelbottom=True, labelleft=False, labelright=True)\n",
        "for spine in ['bottom', 'top', 'right', 'left']:\n",
        "    thumbnail_ax.spines[spine].set_color('red')\n",
        "thumbnail_ax.plot(*query_polygon_3031.exterior.xy, c='r', lw=2)\n",
        "thumbnail_ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e6, right=query_polygon_3031.bounds[2] + 1e6)\n",
        "thumbnail_ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e6, top=query_polygon_3031.bounds[3] + 1e6)\n",
        "thumbnail_ax.set_aspect('equal')\n",
        "ctx.add_basemap(thumbnail_ax, source=ctx.providers.Esri.WorldImagery, zoom=4, crs=aps_epsg_code, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdApU9Tqkx8V"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 3**\n",
        "\n",
        "---\n",
        "\n",
        "**📊 Try It Out: Change the Grid Resolution**\n",
        "\n",
        "Let’s experiment a bit with how the data is gridded.\n",
        "\n",
        "Change the value of the variable `grid_resolution_m` to `5000` (which is 5 km).\n",
        "\n",
        "You can do this in the cell above and then re-run the cell and take a look at the output!\n",
        "\n",
        "> **Question**: What do you observe? 👀\n",
        "> - How does the gridded data look different compared to before?\n",
        "> - Does changing the grid size affect the level of detail?\n",
        "\n",
        "Take a moment to explore, and jot down anything you notice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VEgfbkUYK2C"
      },
      "source": [
        "## POCA vs Swath: Not Just A Difference in Data Volume!\n",
        "CryoSat-2's SIRAL radar works at **Ku-band** (~2.2 cm wavelength), which can penetrate into dry snow.\n",
        "\n",
        " - **POCA** retrieves the **strongest radar echo**, usually from the very top of the snowpack - especially when the surface is smooth and flat.\n",
        "\n",
        "- **Swath** processing combines returns from a wider area using interferometry. This means it integrates **multiple weaker reflections**, not just the brightest one, often capturing signals from slightly deeper layers within the snowpack.\n",
        "\n",
        "So even at the same spot, POCA and Swath can give slightly **different elevation values** because they’re sensing **different depths** in the snow!\n",
        "\n",
        "Lets grid both the Swath and POCA data, and look at their elevation differences!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "bySW7T7oYK2C",
        "outputId": "1b6b1f55-2dbd-47b3-c57d-581f16e99f17"
      },
      "outputs": [],
      "source": [
        "# grid the data on a regular grid with 2km resolution - slightly resolution due to the lower POCA data volume\n",
        "grid_resolution_m = 2000\n",
        "gridded_poca_data_3031 = grid_data(poca_point_data.to_crs(aps_epsg_code), 'elevation', grid_resolution_m)\n",
        "gridded_swath_data_3031 = grid_data(swath_point_data.to_crs(aps_epsg_code), 'elevation', grid_resolution_m)\n",
        "\n",
        "# join the swath and poca data for comparison\n",
        "gridded_data_joined = gridded_poca_data_3031.sjoin(gridded_swath_data_3031, how='left')\n",
        "\n",
        "# calculate the elevation difference between the two grids\n",
        "gridded_data_joined['elevation_difference'] = gridded_data_joined['median_elevation_left'] - gridded_data_joined['median_elevation_right']\n",
        "\n",
        "# remove outliers - gridding directly on Elevation can be quite noisy, especially when considering large spatial extents such as 2km\n",
        "# think about how much the terrain in Edinburgh changes within a similar area!\n",
        "gridded_data_joined = gridded_data_joined[gridded_data_joined['elevation_difference'].abs() < 10]\n",
        "\n",
        "# now plot the elevation differences\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "gridded_data_joined.plot(\n",
        "    'elevation_difference', ax=ax,\n",
        "    vmin=-15, vmax=15, cmap='RdYlBu',\n",
        "    cax=cax, legend=True, legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "fig.get_axes()[1].set_ylabel('Elevation Difference (POCA - Swath) [m]', rotation=-90, fontsize=20, labelpad=30)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=aps_epsg_code, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfMB779nkWHy"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 4**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**📐 Calculating the Median Elevation Difference**\n",
        "\n",
        "Now that we’ve calculated the elevation difference between POCA and Swath elevations (stored in the `elevation_difference` column of `gridded_data_joined` which is a GeoPandas dataframe), let’s take it a step further.\n",
        "\n",
        "> **Question**:\n",
        "Can you calculate the *median* elevation difference across all grid cells?\n",
        "\n",
        "This will give you an idea of the typical offset between the two measurement types.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "Pandas has some handy built-in statistical functions that can help you with this.  \n",
        "You can read about how to calculate the median in Pandas here:  \n",
        "📚 <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.median.html\" target=\"_blank\">pandas.Series.median() documentation</a>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbDt9S19jdbi"
      },
      "source": [
        "## Query the CryoTEMPO-EOLIS Gridded product\n",
        "\n",
        "The CryoTEMPO-EOLIS dataset comes in two forms: one is the **Point Product**, queried and manually gridded above, and another one is the **Gridded Product**.\n",
        "\n",
        "The CryoTEMPO-EOLIS Gridded Product is derived from the Point Product, with an algorithm which is roughly similar to the one defined in the function above.\n",
        "\n",
        "Let's obtain one year (all 2013) of gridded data over the same area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot44-eclut36",
        "outputId": "11017abc-d372-4971-9b69-e74f4e2cee0e"
      },
      "outputs": [],
      "source": [
        "# this is the name of the dataset we are going to use\n",
        "dataset_name = 'CryoTEMPO-EOLIS Gridded Product'\n",
        "\n",
        "swath_gridded_dataset = available_datasets[\n",
        "    available_datasets['dataset_name'] == dataset_name].iloc[0]\n",
        "\n",
        "query_start_time = datetime(2013, 1, 1)\n",
        "query_end_time = datetime(2014, 1, 1)\n",
        "\n",
        "swath_gridded_data, swath_gridded_data_sources = client.query_dataset(\n",
        "    dataset_id=swath_gridded_dataset['dataset_id'],\n",
        "    epsg4326_polygon=query_polygon_4326,\n",
        "    min_datetime=query_start_time,\n",
        "    max_datetime=query_end_time)\n",
        "print(f'This query returned {len(swath_gridded_data)} grid cells, drawn from {len(swath_gridded_data_sources)} original sources.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltXdGtYDzfiV"
      },
      "source": [
        "Now let's select one month of data (**January 2013**) within that year and plot the elevation and the associated uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CHBgqkG86NuT",
        "outputId": "c80e7358-5675-4213-86bd-eff240605c46"
      },
      "outputs": [],
      "source": [
        "# update geometries to show grid cell extent\n",
        "half_res = swath_gridded_data_sources[0]['source_information']['geospatial_resolution'] / 2\n",
        "swath_gridded_data['geometry'] = box(\n",
        "    swath_gridded_data.x_bnds_min,\n",
        "    swath_gridded_data.y_bnds_min,\n",
        "    swath_gridded_data.x_bnds_max,\n",
        "    swath_gridded_data.y_bnds_max\n",
        ")\n",
        "\n",
        "# extract the data for January 2013\n",
        "swath_gridded_data_january = swath_gridded_data.loc[swath_gridded_data.timestamp == datetime(2013, 1, 15).timestamp()]\n",
        "\n",
        "# create a plot with two subplots, one for the elevation and one for the uncertainty on the elevation\n",
        "fig, axes = plt.subplots(ncols=1, nrows=2, sharex=True, sharey=True, figsize=(15, 20))\n",
        "\n",
        "# first subplot\n",
        "ax = axes[0]\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "# plot the elevation data\n",
        "elevation_plot = swath_gridded_data_january.plot(\n",
        "    'elevation', ax=ax, cmap='viridis', cax=cax, legend=True,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "\n",
        "# setup the colorbar\n",
        "fig.get_axes()[2].set_ylabel('gridded elevation  [m]', fontsize=20, rotation=-90, labelpad=30)\n",
        "\n",
        "# bounds, axes labels and background map\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=aps_epsg_code, attribution=False)\n",
        "\n",
        "# second subplot\n",
        "ax = axes[1]\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "# plot the elevation data\n",
        "uncertainty_plot = swath_gridded_data_january.plot(\n",
        "    'uncertainty', ax=ax, cmap='Greens', cax=cax, legend=True,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "\n",
        "# setup the colorbar\n",
        "fig.get_axes()[3].set_ylabel('uncertainty  [m]', fontsize=20, rotation=-90, labelpad=30)\n",
        "\n",
        "# bounds, axes labels and background map\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=aps_epsg_code, attribution=False)\n",
        "\n",
        "# bring the subplots a little closer to each other\n",
        "fig.subplots_adjust(hspace=0.07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B2wNO0k_s0H"
      },
      "source": [
        "Now extract the gridded data for <u>**December 2013**</u> and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "4NLSmMv2sVpW",
        "outputId": "1ea56f06-1189-4510-c983-5460fb479e09"
      },
      "outputs": [],
      "source": [
        "swath_gridded_data_december = swath_gridded_data.loc[swath_gridded_data.timestamp == datetime(2013, 12, 15).timestamp()]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "elevation_plot = swath_gridded_data_december.plot(\n",
        "    'elevation', ax=ax, cmap='viridis', cax=cax, legend=True,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "fig.get_axes()[1].set_ylabel('gridded elevation  [m]', fontsize=20, rotation=-90, labelpad=30)\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=3031, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFGTTjOdAGwl"
      },
      "source": [
        "## Elevation change\n",
        "\n",
        "It's hard to see any change between the two datasets, given the range of elevation measurements that goes from 300 m to 1300 m. Let's then calculate the difference between the two datasets, grid cell by grid cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLqs04TStKcl"
      },
      "outputs": [],
      "source": [
        "# first, merge the datasets such that the data associated to a given grid cell is all on the same row\n",
        "# the columns with the same name in the two datasets, for example \"elevation\" or \"uncertainty\", will have a suffix appended to their name to distinguish them\n",
        "swath_gridded_data_merged = pd.merge(swath_gridded_data_january, swath_gridded_data_december, on=['x', 'y'], how='inner', suffixes=['_jan', '_dec'])\n",
        "\n",
        "# the two \"geometry\" columns are exactly the same, just select one of the two as the \"geometry\" to use in the plotting\n",
        "swath_gridded_data_merged.set_geometry('geometry_jan', inplace=True)\n",
        "\n",
        "# calculate the elevation change between December 2013 and January 2013\n",
        "swath_gridded_data_merged['elevation_change'] = swath_gridded_data_merged.elevation_dec - swath_gridded_data_merged.elevation_jan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "vc6Yv-QuCbrc",
        "outputId": "5038ebcc-bd95-44ee-c4ff-126c4ed88a55"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "elevation_change_plot = swath_gridded_data_merged.plot(\n",
        "    'elevation_change', ax=ax, cmap='RdYlBu', cax=cax, legend=True, vmin=-10, vmax=10,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "fig.get_axes()[1].set_ylabel('Elevation Change  [m]', fontsize=20, rotation=-90, labelpad=30)\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=3031, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1jIp1crol1S"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 5**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**🌍 Investigating Significant Elevation Drops**\n",
        "\n",
        "You now have a GeoPandas dataframe called `swath_gridded_data_merged`, which contains:\n",
        "\n",
        "- A column called `elevation_change`  \n",
        "  → This shows the difference in elevation between **December 2013** and **January 2013**  \n",
        "  → It was calculated by subtracting January elevations from December elevations  \n",
        "\n",
        "> **Question**:  \n",
        "How many grid cells report an elevation change **lower than -10 metres** over this time period?  \n",
        "And what is the **total area** covered by these grid cells?\n",
        "\n",
        "These might indicate areas with unusually large elevation losses, which could be important for further analysis.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "\n",
        "To answer this:\n",
        "\n",
        "1. **Filter the GeoDataFrame** to find rows where `elevation_change < -10`  \n",
        "2. Use the `.area` attribute of the geometry column to calculate each grid cell's area (this will be in metres squared due to our choice of coordinate reference system!)\n",
        "3. Use `.sum()` to get the **total** area of those cells\n",
        "\n",
        "Useful links:\n",
        "- <a href=\"https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing\" target=\"_blank\">How to filter a DataFrame using Boolean indexing (Pandas)</a>  \n",
        "- <a href=\"https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.area.html\" target=\"_blank\">.area attribute docs (GeoPandas)</a>\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAepuYosELxx"
      },
      "source": [
        "**What’s Going On Here?**\n",
        "\n",
        "Something interesting is happening in the data!\n",
        "\n",
        "We’re seeing significant elevation changes in a relatively short time:\n",
        "\n",
        "- In the center of the plot, there’s a drop of more than **12 meters in less than a year** - shown by the dark red patch.\n",
        "\n",
        "- Similar patterns (though less extreme, around 4–5 meters) appear in two other regions during 2013.\n",
        "\n",
        "Let's dive more into where these changes are coming from!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtQYpeVQE9L3"
      },
      "source": [
        "# Subglacial lakes\n",
        "\n",
        "Here we are observing changes that are not happening on the surface, but **beneath** the ice sheet.\n",
        "At the boundary between ice and the underlying bedrock, liquid water can exist due to the high pressure. This water forms a network of rivers and lakes that experience cycles of recharge and discharge. During these cycles, the surface elevation of the ice sheet changes accordingly, for example with large localised apparent thinning during discharge events.\n",
        "\n",
        "The areas with intense negative signal in the map above correspond to some of the subglacial lakes belonging to the network beneath the Thwaites glacier, that went through an intense discharge event during 2013, studied in scientific publications such as [Malczyk et al. (2020)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020GL089658), [Malczyk et al. (2023)](https://www.cambridge.org/core/journals/journal-of-glaciology/article/constraints-on-subglacial-melt-fluxes-from-observations-of-active-subglacial-lake-recharge/06FB1F23816324D75FE46141C5DF4014) and [Gourmelen et al. (2025)](https://www.nature.com/articles/s41467-025-57417-1).\n",
        "\n",
        "Let's have a closer look at these subglacial lakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSh6QCDApD-p"
      },
      "source": [
        "## Download the file containing the subglacial lakes extent\n",
        "\n",
        "Thanks to studies such as Malczyk et al. (2020), the locations of these lakes are relatively well known. The polygons corresponding to the extent of the subglacial lakes reported in Malczyk et al. (2020) are stored in a file on Google Drive. Run the next cell to download this file (\"1UVFhKVDVPndr5HVoadE7lVeWZNDuAEOj\" is the ID of the file).\n",
        "\n",
        "If you click on the \"Files\" icon in the left bar, you will see the file in the local storage associated with this Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9ISzkgnDHE",
        "outputId": "88975ed2-d096-4dc0-967b-1ccfffe20644"
      },
      "outputs": [],
      "source": [
        "! gdown 1UVFhKVDVPndr5HVoadE7lVeWZNDuAEOj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4ZX3Fc5F8J"
      },
      "source": [
        "## Overlay the lakes extent onto the gridded elevation change map\n",
        "\n",
        "First load the file.\n",
        "\n",
        "The column `geometry` contains the extent of each lake, expressed in projected coordinates (EPSG:3031).\n",
        "\n",
        "The column `lake_id` contains the names of the lakes, where the number indicates the distance of the lake from the grounding line of Thwaites Glacier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "hFBnTfVUnNwV",
        "outputId": "94be17c3-69a9-4f3b-f7e2-86d46fafeb40"
      },
      "outputs": [],
      "source": [
        "lakes_extent_file_path = os.path.join(os.getcwd(), 'malczyk2020_lakes_extent.gpkg')\n",
        "lakes_extent_df = gpd.read_file(lakes_extent_file_path)\n",
        "lakes_extent_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_C1ggko58mP"
      },
      "source": [
        "The lakes nicely match the areas with large thinning we observed in the elevation change map we generated earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "w4FDb1iHh8FR",
        "outputId": "ff6ea174-41f7-4bd7-b114-d3550d0ee15b"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='4%', pad=0.15)\n",
        "\n",
        "elevation_change_plot = swath_gridded_data_merged.plot(\n",
        "    'elevation_change', ax=ax, cmap='RdYlBu', cax=cax, legend=True, vmin=-10, vmax=10,\n",
        "    legend_kwds={'orientation': 'vertical', 'shrink': .8, 'pad': .1})\n",
        "\n",
        "for lake_index, lake_data in lakes_extent_df.iterrows():\n",
        "    lake_polygon = lake_data.geometry\n",
        "    ax.plot(*lake_polygon.exterior.xy, c='g')\n",
        "    for ring in lake_polygon.interiors:\n",
        "        ax.plot(*ring.xy, c='g')\n",
        "    ax.text(x=lake_polygon.centroid.x + np.power(-1, lake_index) * 2e4,\n",
        "            y=lake_polygon.centroid.y - np.power(-1, lake_index) * 2e4,\n",
        "            s=lake_data.lake_id, fontsize=14, c='g', horizontalalignment='center', verticalalignment='center')\n",
        "\n",
        "fig.get_axes()[1].set_ylabel('Elevation Change  [m]', fontsize=20, rotation=-90, labelpad=30)\n",
        "ax.set_xlim(left=query_polygon_3031.bounds[0] - 1e5, right=query_polygon_3031.bounds[2] + 1e5)\n",
        "ax.set_ylim(bottom=query_polygon_3031.bounds[1] - 1e5, top=query_polygon_3031.bounds[3] + 1e5)\n",
        "ax.set_ylabel('Northing  [m]', fontsize=20)\n",
        "ax.set_xlabel('Easting  [m]', fontsize=20)\n",
        "ax.set_aspect('equal')\n",
        "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, zoom=6, crs=3031, attribution=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRQEpB6s6Twl"
      },
      "source": [
        "## Time series of volume change for each subglacial lake\n",
        "\n",
        "Now let's use CryoTEMPO-EOLIS Gridded Product to derive the **volume change** of each lake.\n",
        "\n",
        "Remember, CryoSat-2 allows us to measure elevation changes at the surface. Knowing the extent of the lakes under the ice sheet, we can use the area and the elevation change to calculate the volume of water that has drained (during the discharge events) or that the lake has gained (during the recharge periods).\n",
        "\n",
        "First of all, let's calculate the **time series** of mean surface elevation change. CryoTEMPO-EOLIS Gridded Product is a monthly product containing the ice surface elevation on a 2km regular grid. Each lake, having areas of a few hundreds km$^2$, contains multiple grid cells.\n",
        "\n",
        "In the function defined below, for each month we calculate, grid cell by grid cell, the difference to the first available month, used as a reference elevation / first timestep. Subsequently, we take the monthly average of the elevation difference, weighting by the associated uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM3bOPfT_c0m"
      },
      "outputs": [],
      "source": [
        "from datetime import timezone\n",
        "\n",
        "def calculate_elevation_change_time_series(gridded_product_data: gpd.GeoDataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate a weighted average elevation change per month and associated errors.\n",
        "\n",
        "    Weights are assigned using the CryoTEMPO-EOLIS gridded product uncertainty.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    gridded_product_data : gpd.GeoDataFrame\n",
        "        CryoTEMPO-EOLIS Gridded Product data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing the weighted average elevation change, the associated error and the date of each timestep.\n",
        "    \"\"\"\n",
        "    unique_timestamps = sorted(gridded_product_data.timestamp.unique())\n",
        "\n",
        "    # elevation change is in reference to first gridded product\n",
        "    reference_dem = gridded_product_data[gridded_product_data.timestamp == unique_timestamps[0]]\n",
        "\n",
        "    # initialise empty lists for time series data\n",
        "    weighted_mean_timeseries = []\n",
        "    weighted_mean_errors = []\n",
        "    for unique_timestamp in unique_timestamps:\n",
        "        gridded_product_for_timestamp = gridded_product_data[gridded_product_data.timestamp == unique_timestamp]\n",
        "        merged_gdf = reference_dem.sjoin(df=gridded_product_for_timestamp, how='inner')\n",
        "        merged_gdf['elevation_difference'] = merged_gdf['elevation_right'] - merged_gdf['elevation_left']\n",
        "\n",
        "        # combine uncertainties for elevation difference\n",
        "        merged_gdf['elevation_difference_unc'] = np.sqrt(\n",
        "            merged_gdf['uncertainty_right']**2 + merged_gdf['uncertainty_left']**2)\n",
        "\n",
        "        # calculate average elevation change, weighted by measurement uncertainty\n",
        "        weighted_mean = (np.sum(merged_gdf['elevation_difference'] / merged_gdf['elevation_difference_unc']**2)\n",
        "                        / np.sum(1 / merged_gdf['elevation_difference_unc']**2))\n",
        "\n",
        "        # calculate weighted average uncertainty\n",
        "        error = np.sqrt(1 / np.sum(1 / merged_gdf['elevation_difference_unc']**2))\n",
        "\n",
        "        weighted_mean_timeseries.append(weighted_mean)\n",
        "        weighted_mean_errors.append(error)\n",
        "\n",
        "    dates = [datetime.fromtimestamp(ts, tz=timezone.utc) for ts in unique_timestamps]\n",
        "\n",
        "    return pd.DataFrame.from_dict({\n",
        "        'date': dates,\n",
        "        'timestamp': [d.timestamp() for d in dates],\n",
        "        'mean_elevation_change': weighted_mean_timeseries,\n",
        "        'uncertainty': weighted_mean_errors})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Y9FEz-zlmL"
      },
      "source": [
        "### Query of CryoTEMPO-EOLIS gridded data over each lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msrbWgNT1Dtw",
        "outputId": "c097a2fc-5856-4cdc-aaeb-fe036fa76d9f"
      },
      "outputs": [],
      "source": [
        "# info regarding the dataset\n",
        "dataset_name = 'CryoTEMPO-EOLIS Gridded Product'\n",
        "available_datasets = client.list_datasets()\n",
        "eolis_gridded_product_dataset = available_datasets[\n",
        "    available_datasets['dataset_name'] == dataset_name].iloc[0]\n",
        "\n",
        "# these dictionaries is where we'll save the data returned by the query\n",
        "eolis_gridded_data_over_lakes = {}\n",
        "sources_eolis_gridded_data_over_lakes = {}\n",
        "\n",
        "# in order to query data from Specklia within an area, the corresponding polygon needs to be in EPSG 4326 (longitude-latitude)\n",
        "lakes_extent_4326_df = lakes_extent_df.to_crs(epsg=4326)\n",
        "\n",
        "# time span of the query: monthly data from January 2011 to August 2023\n",
        "query_start_time = datetime(2011, 1, 1)\n",
        "query_end_time = datetime(2023, 9, 1)\n",
        "\n",
        "# run the queries (one per lake) - in total it should take 1-2 minutes\n",
        "for _, lake_data in lakes_extent_4326_df.iterrows():\n",
        "    eolis_gridded_data_over_lakes[lake_data.lake_id], sources_eolis_gridded_data_over_lakes[lake_data.lake_id] = client.query_dataset(\n",
        "        dataset_id=eolis_gridded_product_dataset['dataset_id'],\n",
        "        epsg4326_polygon=lake_data.geometry,\n",
        "        min_datetime=query_start_time,\n",
        "        max_datetime=query_end_time,\n",
        "        columns_to_return=['timestamp', 'elevation', 'uncertainty'])\n",
        "\n",
        "    print(f'The query of subglacial lake with ID {lake_data.lake_id} returned '\n",
        "          f'{len(eolis_gridded_data_over_lakes[lake_data.lake_id])} grid cells, '\n",
        "          f'drawn from {len(sources_eolis_gridded_data_over_lakes[lake_data.lake_id])} original sources.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE-GFGQxGBh2"
      },
      "source": [
        "### Elevation change time series\n",
        "\n",
        "Use the `calculate_elevation_change_time_series` function on the gridded data and then plot the resulting time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep1EWZwe-8HQ"
      },
      "outputs": [],
      "source": [
        "elevation_change_time_series_over_lakes = {}\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "    elevation_change_time_series_over_lakes[lake_data.lake_id] = calculate_elevation_change_time_series(\n",
        "        eolis_gridded_data_over_lakes[lake_data.lake_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "feVA2Y71G3Vk",
        "outputId": "634f606b-1546-43ab-9277-3bc9f023eeef"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    time_series_df = elevation_change_time_series_over_lakes[lake_data.lake_id]\n",
        "    ax.plot(time_series_df.date, time_series_df.mean_elevation_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(time_series_df.date,\n",
        "                    time_series_df.mean_elevation_change - time_series_df.uncertainty,\n",
        "                    time_series_df.mean_elevation_change + time_series_df.uncertainty,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Cumulative Elevation Change  [m]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GfFeOMxwyye"
      },
      "source": [
        "The plot above is extremely interesting! The main features of the temporal evolution of these lakes are visible:\n",
        "- the **discharge event** of 2013, that involved all four lakes;\n",
        "- Thw_124, the largest subglacial lake of the network, rapidly **recharging** between 2015 and 2018;\n",
        "- the **second discharge event**, in 2017, that involved Thw_142 and Thw_170."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oKddK89YCiS"
      },
      "source": [
        "### Query of CryoTEMPO-EOLIS gridded data in the surrounding area\n",
        "\n",
        "However, another step is needed. Remember, while the lakes recharge and discharge, the ice sheet above can also thicken or thin due to the larger scale evolution (and sadly Thwaites glacier is thinning quite fast).\n",
        "\n",
        "Hence, the elevation change above is actually given by two contributions: the **discharge-recharge cycle** of the lakes and the **thinning of the ice sheet**. Now let's try to disentangle the two processes and derive the net volume change only due to the lakes.\n",
        "\n",
        "The strategy is the following:\n",
        "1. for each lake, we consider an area that is a bit larger than the original lake extent (in this case, 10km wider in all directions)\n",
        "2. query again the EOLIS gridded data, but eliminate all the grid cells that are within the lake extent\n",
        "3. calculate the mean elevation change time series for the surrounding area of each lake\n",
        "4. the difference between the mean elevation change calculated in the previous step and time series of the subglacial lake be isolated signal of the lake dynamics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahrWcaFGYIyu"
      },
      "outputs": [],
      "source": [
        "lakes_with_buffer_extent_df = lakes_extent_df.copy()\n",
        "# let's use the 'buffer' method to extend the query area around each lake\n",
        "lakes_with_buffer_extent_df['geometry'] = lakes_extent_df.geometry.buffer(10000)\n",
        "# remember to convert everything to longitude-latitude\n",
        "lakes_with_buffer_extent_4326_df = lakes_with_buffer_extent_df.to_crs(epsg=4326)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN-e4WzrZFIc",
        "outputId": "5c789ebb-9631-4023-a82e-a993cfe047db"
      },
      "outputs": [],
      "source": [
        "# we save the grid cells over lake+buffer here\n",
        "eolis_gridded_data_with_buffer = {}\n",
        "sources_eolis_gridded_data_with_buffer = {}\n",
        "\n",
        "# and the grid cells in the area surrounding each lake here\n",
        "eolis_gridded_data_only_buffer = {}\n",
        "\n",
        "for _, lake_data in lakes_with_buffer_extent_4326_df.iterrows():\n",
        "    eolis_gridded_data_with_buffer[lake_data.lake_id], sources_eolis_gridded_data_with_buffer[lake_data.lake_id] = client.query_dataset(\n",
        "        dataset_id=eolis_gridded_product_dataset['dataset_id'],\n",
        "        epsg4326_polygon=lake_data.geometry,\n",
        "        min_datetime=query_start_time,\n",
        "        max_datetime=query_end_time,\n",
        "        columns_to_return=['timestamp', 'elevation', 'uncertainty'])\n",
        "\n",
        "    print(f'The query of subglacial lake + buffer with ID {lake_data.lake_id} returned '\n",
        "          f'{len(eolis_gridded_data_with_buffer[lake_data.lake_id])} grid cells, '\n",
        "          f'drawn from {len(sources_eolis_gridded_data_with_buffer[lake_data.lake_id])} original sources.')\n",
        "\n",
        "    # keep all the grid cells that are not within the lake extent (i.e., that were not returned in the first query)\n",
        "    eolis_gridded_data_only_buffer[lake_data.lake_id] = pd.concat(\n",
        "        [eolis_gridded_data_with_buffer[lake_data.lake_id], eolis_gridded_data_over_lakes[lake_data.lake_id]]).drop_duplicates(keep=False)\n",
        "\n",
        "    # sanity check: the number of points for the buffer zone should match the difference between the second query and the first\n",
        "    assert len(eolis_gridded_data_only_buffer[lake_data.lake_id]) == (len(eolis_gridded_data_with_buffer[lake_data.lake_id]) - len(eolis_gridded_data_over_lakes[lake_data.lake_id]))\n",
        "    print(f'The buffer around lake with ID {lake_data.lake_id} '\n",
        "          f'contains {len(eolis_gridded_data_only_buffer[lake_data.lake_id])} grid cells, as expected.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fRevgtYUBbS"
      },
      "source": [
        "### Elevation change time series in the surrounding area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHzFN6BUGTA"
      },
      "outputs": [],
      "source": [
        "elevation_change_time_series_over_buffer = {}\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "    elevation_change_time_series_over_buffer[lake_data.lake_id] = calculate_elevation_change_time_series(\n",
        "        eolis_gridded_data_only_buffer[lake_data.lake_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "p1J6UETtUg6I",
        "outputId": "cee9646d-c6ec-4c41-c7cc-5c07a62985c3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    time_series_df = elevation_change_time_series_over_buffer[lake_data.lake_id]\n",
        "    ax.plot(time_series_df.date, time_series_df.mean_elevation_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(time_series_df.date,\n",
        "                    time_series_df.mean_elevation_change - time_series_df.uncertainty,\n",
        "                    time_series_df.mean_elevation_change + time_series_df.uncertainty,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('date')\n",
        "ax.set_ylabel('cumulative elevation change  [m]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_PVceg5uh4V"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 6**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**🧊 Subglacial Lakes and Surface Thinning**\n",
        "\n",
        "Let’s dig into the spatial patterns in elevation change around subglacial lakes.\n",
        "\n",
        "> **Question**:  \n",
        "In the area surrounding which **subglacial lake** has there been the most thinning?  \n",
        "Do you notice any relationship between **elevation change** and the **proximity to the ice sheet margin**?\n",
        "\n",
        "These patterns can tell us something about the underlying processes driving surface changes.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "\n",
        "Have a look at the map shown a few cells above to locate the subglacial lakes.  \n",
        "The **lake IDs** are a clue: the number in the ID indicates how far each lake is from the **ice sheet margin** (lower numbers = closer to the edge).\n",
        "\n",
        "Try comparing elevation change around lakes with lower ID numbers vs higher ones. See any trends?\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>📉 Click to reveal insights underlying processes driving surface change </summary>\n",
        "\n",
        "**Why Is Surface Thinning Stronger Near the Margins of an Ice Sheet?**\n",
        "\n",
        "---\n",
        "\n",
        "🌊 **Proximity to Ocean & Warmer Air**\n",
        "\n",
        "Marginal regions are more exposed to **warm ocean water** and **higher air temperatures**, which increase both basal and surface melting.\n",
        "Thwaites is experiencing **rapid retreat at its grounding line**, which leads to **faster ice flow** and **surface lowering** in adjacent areas.\n",
        "\n",
        "---\n",
        "\n",
        "↘️ **Faster Ice Flow Near the Margins**\n",
        "\n",
        "Ice often flows **more quickly near the edges** due to steeper slopes and reduced friction. This leads to **dynamic thinning** which outpaces accumulation from snowfall, leading to net elevation loss.\n",
        "\n",
        "---\n",
        "\n",
        "🧱 **Loss of Ice Shelf Support**\n",
        "\n",
        "If an ice shelf near the margin **thins or collapses**, glaciers feeding into it can **accelerate**, triggering inland thinning through **loss of buttressing** - this is a key feature in Thwaites!\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhLjJvsT3FU"
      },
      "source": [
        "### Finally, volume change over the subglacial lakes\n",
        "\n",
        "The plot above shows that\n",
        "1. the ice sheet is thinning, hence it's important to remove this thinning signal to get the net elevation change due to the lakes\n",
        "2. the ice sheet thinning is not homogeneous, so comparing the elevation change for the different lakes only based on the first time series would lead to biased results\n",
        "\n",
        "Now, let's calculate the net elevation change due to the lakes' discharge/recharge cycles and convert it to a volume change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX-WDeuRWKvy"
      },
      "outputs": [],
      "source": [
        "elevation_change_time_series = {}\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    elevation_change_time_series_df = pd.merge(\n",
        "        elevation_change_time_series_over_lakes[lake_data.lake_id],\n",
        "        elevation_change_time_series_over_buffer[lake_data.lake_id],\n",
        "        how='inner', on='date', suffixes=['_lake', '_buffer'])\n",
        "    # calculate the net elevation difference of the subglacial lakes and propagate the uncertainty\n",
        "    elevation_change_time_series_df['net_mean_elevation_change'] = \\\n",
        "        elevation_change_time_series_df.mean_elevation_change_lake - elevation_change_time_series_df.mean_elevation_change_buffer\n",
        "    elevation_change_time_series_df['net_uncertainty_mean_elevation_change'] = \\\n",
        "        np.sqrt(elevation_change_time_series_df.uncertainty_lake**2 + elevation_change_time_series_df.uncertainty_buffer**2)\n",
        "    elevation_change_time_series[lake_data.lake_id] = elevation_change_time_series_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "UdO7NDDVXz_c",
        "outputId": "b59fc8a0-411c-4fec-8db0-3b7cd140ce2c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    time_series_df = elevation_change_time_series[lake_data.lake_id]\n",
        "    ax.plot(time_series_df.date, time_series_df.net_mean_elevation_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(time_series_df.date,\n",
        "                    time_series_df.net_mean_elevation_change - time_series_df.net_uncertainty_mean_elevation_change,\n",
        "                    time_series_df.net_mean_elevation_change + time_series_df.net_uncertainty_mean_elevation_change,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('date')\n",
        "ax.set_ylabel('net elevation change  [m]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQDn1YkMfeNV"
      },
      "source": [
        "In order to transform surface elevation change to volume change, a fairly good first-order approximation is to multiply by the area of each subglacial lake.\n",
        "For the latter, we can make use of the `area` attribute of `Polygon` objects.\n",
        "\n",
        "📌 Note: The volume in this context is normally expressed in km$^3$. Surface elevation change is expressed in metres. The lakes extents are in Antarctic Polar Stereographic coordinates, whose units are also metres. Therefore, the area of the polygons will be in m$^2$. Hence the multiplication factor 10$^{-9}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6KTCvhLnHWs"
      },
      "outputs": [],
      "source": [
        "m3_to_km3_conversion_factor = 1e-9\n",
        "\n",
        "volume_change_time_series = {}\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "    elevation_change_df = elevation_change_time_series[lake_data.lake_id]\n",
        "    volume_change_time_series[lake_data.lake_id] = pd.DataFrame.from_dict({\n",
        "        'date': elevation_change_df.date,\n",
        "        'timestamp': elevation_change_df.timestamp_lake,\n",
        "        'volume_change': m3_to_km3_conversion_factor * elevation_change_df.net_mean_elevation_change * lake_data.geometry.area,\n",
        "        'uncertainty_volume_change': m3_to_km3_conversion_factor * elevation_change_df.net_uncertainty_mean_elevation_change * lake_data.geometry.area})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "JdBbwqiJWEC3",
        "outputId": "aafaee91-e74b-4382-b6de-8208cb4ae5f3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    volume_change_df = volume_change_time_series[lake_data.lake_id]\n",
        "    ax.plot(volume_change_df.date, volume_change_df.volume_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(volume_change_df.date,\n",
        "                    volume_change_df.volume_change - volume_change_df.uncertainty_volume_change,\n",
        "                    volume_change_df.volume_change + volume_change_df.uncertainty_volume_change,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('date')\n",
        "ax.set_ylabel(r'lake volume change  [km$^3$]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5csOCfEYv_YA"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 7**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Volume Loss of Subglacial Lake `Thw_124`**\n",
        "\n",
        "The figure above clearly shows the discharge-recharge cycle of subglacial lakes -\n",
        "let’s quantify the volume change of a subglacial lake during the discharge event in 2013!\n",
        "\n",
        "> **Question**:  \n",
        "How much **volume** did the subglacial lake `Thw_124` lose between **April 15th 2013** and **January 15th 2014**?\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for a hint</summary>\n",
        "\n",
        "To find the volume loss:\n",
        "\n",
        "1. **Filter the dataframe** to extract the `volume_change` values for those two dates\n",
        "   → Use the timestamp column to select only the rows corresponding to **April 15th 2013** and **January 15th 2014**\n",
        "   ```python\n",
        "   vol_april_2013 = volume_change_time_series['Thw124'][volume_change_time_series['Thw124']['timestamp'] == datetime(2013, 4, 15, tzinfo=timezone.utc).timestamp()]['volume_change'].values[0]\n",
        "   ```\n",
        "\n",
        "2. **Calculate the difference**:  \n",
        "   `vol_january_2014 - vol_april_2013`  \n",
        "   This gives the **net volume change** between the two dates.\n",
        "\n",
        "📌 If the result is negative, it indicates **volume loss** over that time period.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QUFq3f3xecK"
      },
      "source": [
        "---\n",
        "\n",
        "**❓ Question 8**\n",
        "\n",
        "---\n",
        "\n",
        "**📈 Challenge: Estimating Recharge Rate of a Subglacial Lake**\n",
        "\n",
        "Let’s dive into some data analysis!\n",
        "\n",
        "> **Question**:  \n",
        "What was the **average recharge rate** of **Thw_124** between **May 2014** and **March 2017**?  \n",
        "Express your answer in **km³ per year**.\n",
        "\n",
        "<details>\n",
        "<summary>💡 Click here for hints</summary>\n",
        "\n",
        "To estimate the recharge rate:\n",
        "\n",
        "1. **Filter the volume change dataframe** for lake Thw_124 (`volume_change_time_series['Thw_124']`) to extract dates between May 2014 and March 2017\n",
        "   ```python\n",
        "    filtered_df = volume_change_time_series['Thw124'][(volume_change_time_series['Thw124']['timestamp'] >= datetime(2014, 5, 15, tzinfo=timezone.utc).timestamp()) & (volume_change_time_series['Thw124']['timestamp'] <= datetime(2017, 3, 15, tzinfo=timezone.utc).timestamp())]\n",
        "    ```\n",
        "\n",
        "2. **Fit a linear trend** to the volume change time series for Thw_124 using the **least squares method**.\n",
        "   - Try using [`numpy.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) to fit a straight line: `volume = rate × time + offset`\n",
        "   - The slope of that line will give you an **average rate of change** in volume.\n",
        "   - Note: you should use the timestamp column for you time axis.\n",
        "\n",
        "3. **Convert the result to km³/yr**\n",
        "   - Make sure the **volume units** are in **km³**\n",
        "   - Check the **time axis**: the timestamps in units of seconds - convert accordingly to get a result in **per year**\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhjJaOc7aeTP"
      },
      "source": [
        "Have a look at Figure 3 in [Malczyk et al. (2023)](https://www.cambridge.org/core/journals/journal-of-glaciology/article/constraints-on-subglacial-melt-fluxes-from-observations-of-active-subglacial-lake-recharge/06FB1F23816324D75FE46141C5DF4014). It looks fairly similar, doesn't it?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "oggm_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
