{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CryoTEMPO: Assessing Subglacial Lakes Dynamics in the Thwaites area, Westerm Antarctica"
      ],
      "metadata": {
        "id": "osWavvIYpHpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use CryoTEMPO-EOLIS gridded product and CryoTEMPO Land Ice to study the evolution of the surface elevation of four subglacial lakes within the Thwaites glacier, in Western Antarctica."
      ],
      "metadata": {
        "id": "AkVKaJMppZPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup\n",
        "To run this notebook, you will need to make sure that the folllowing packages are installed in your python environment (all can be installed via pip/conda):\n",
        "- matplotlib\n",
        "- geopandas\n",
        "- contextily\n",
        "- ipykernel\n",
        "- shapely\n",
        "- specklia\n",
        "\n",
        "If you are using the Google Colab environment, these packages will be installed in the next cell. Please note this step may take a few minutes."
      ],
      "metadata": {
        "id": "S4PirYISzqRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    %pip install rasterio --no-binary rasterio\n",
        "    %pip install specklia\n",
        "    %pip install matplotlib\n",
        "    %pip install geopandas\n",
        "    %pip install contextily\n",
        "    %pip install shapely\n",
        "    %pip install python-dotenv"
      ],
      "metadata": {
        "id": "WT0m7yFmnjPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaYLPUORnNwT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from time import perf_counter\n",
        "from datetime import datetime, timedelta\n",
        "import geopandas as gpd\n",
        "import math\n",
        "import pandas as pd\n",
        "from shapely import Point\n",
        "\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "from matplotlib import cm\n",
        "\n",
        "import contextily as ctx\n",
        "import matplotlib.pyplot as plt\n",
        "from specklia import Specklia\n",
        "\n",
        "# local EPSG projection code for plotting, we're considering data in the southern hemisphere so will use the\n",
        "# Southern Polar Stereographic projection\n",
        "sps_epsg_code = 3031\n",
        "\n",
        "# To run this code yourself, first generate your own key using https://specklia.earthwave.co.uk.\n",
        "if 'DEMO_API_KEY' in os.environ:\n",
        "    client = Specklia(os.environ['DEMO_API_KEY'])\n",
        "else:\n",
        "    user_api_key = input('Please generate your own key using https://specklia.earthwave.co.uk/ApiKeys and paste it here:')\n",
        "    client = Specklia(user_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the file containing the subglacial lakes extent\n",
        "\n",
        "The polygons describing the extent of the subglacial lakes are stored in a file on Google Drive. Its ID is \"1kByOjALzLrxueYHHgSSrfgjRI_73B9tT\".\n",
        "Run the next cell to download this file. If you click on the \"Files\" icon in the left bar, you will see that now the file is present in the local storage associated with this Colab session."
      ],
      "metadata": {
        "id": "BSh6QCDApD-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown 1kByOjALzLrxueYHHgSSrfgjRI_73B9tT"
      ],
      "metadata": {
        "id": "fb_d_GLqt5if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the file and plot the polygons on a map\n",
        "\n",
        "NB: the lake extents are expressed in projected coordinates (EPSG:3031)."
      ],
      "metadata": {
        "id": "Gj4ZX3Fc5F8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFBnTfVUnNwV"
      },
      "outputs": [],
      "source": [
        "lakes_extent_file_path = os.path.join(os.getcwd(), 'thwaites_sub_glacial_lake_mask.gpkg')\n",
        "lakes_extent_df = gpd.read_file(lakes_extent_file_path)\n",
        "lakes_extent_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "    lake_polygon = lake_data.geometry\n",
        "    ax.plot(*lake_polygon.exterior.xy, c='r')\n",
        "    for ring in lake_polygon.interiors:\n",
        "        ax.plot(*ring.xy, c='r')\n",
        "    ax.text(x=lake_polygon.centroid.x - 4e4,\n",
        "            y=lake_polygon.centroid.y + 4e4,\n",
        "            s=lake_data.lake_id)\n",
        "ax.set_xlim(left=-1.6e6, right=-1.3e6)\n",
        "ax.set_ylim(bottom=-5.5e5, top=-2.5e5)\n",
        "ax.set_ylabel('Y  [m]')\n",
        "ax.set_xlabel('X  [m]')\n",
        "ctx.add_basemap(\n",
        "    ax, source=ctx.providers.Esri.WorldImagery, crs=sps_epsg_code,\n",
        "    attribution=False, zoom=5)"
      ],
      "metadata": {
        "id": "c6SEp-OI5Vd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_teVf_qnNwW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "for lake_polygon in lakes_extent_df.geometry:\n",
        "  ax.plot(*lake_polygon.exterior.xy, c='r')\n",
        "  for ring in lake_polygon.interiors:\n",
        "      ax.plot(*ring.xy, c='r')\n",
        "ax.set_xlim(left=-1.8e6, right=-1.2e6)\n",
        "ax.set_ylim(bottom=-7e5, top=-1e5)\n",
        "ax.set_ylabel('Y  [m]')\n",
        "ax.set_xlabel('X  [m]')\n",
        "ctx.add_basemap(\n",
        "    ax, source=ctx.providers.Esri.WorldImagery, crs=sps_epsg_code,\n",
        "    attribution=False, zoom=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the CryoTEMPO-EOLIS gridded data for each subglacial lake"
      ],
      "metadata": {
        "id": "i0Y9FEz-zlmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to query data from Specklia within an area, the corresponding polygon needs to be in EPSG 4326 (longitude-latitude)\n",
        "\n",
        "lakes_extent_4326_df = lakes_extent_df.to_crs(epsg=4326)"
      ],
      "metadata": {
        "id": "msrbWgNT1Dtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'CryoTEMPO-EOLIS Gridded Product'\n",
        "available_datasets = client.list_datasets()\n",
        "gridded_product_dataset = available_datasets[\n",
        "    available_datasets['dataset_name'] == dataset_name].iloc[0]"
      ],
      "metadata": {
        "id": "ep1EWZwe-8HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lake_gridded_eolis_data = {}\n",
        "lake_gridded_eolis_sources = {}\n",
        "\n",
        "for _, lake_data in lakes_extent_4326_df.iterrows():\n",
        "    query_start_time = perf_counter()\n",
        "    lake_gridded_eolis_data[lake_data.lake_id], lake_gridded_eolis_sources[lake_data.lake_id] = client.query_dataset(\n",
        "        dataset_id=gridded_product_dataset['dataset_id'],\n",
        "        epsg4326_polygon=lake_data.geometry,\n",
        "        min_datetime=datetime(2011, 1, 1),\n",
        "        max_datetime=datetime(2023, 9, 1),\n",
        "        columns_to_return=['timestamp', 'elevation', 'uncertainty'])\n",
        "\n",
        "    print(f'Query of subglacial lake with ID {lake_data.lake_id} complete in '\n",
        "          f'{perf_counter()-query_start_time:.2f} seconds, '\n",
        "          f'{len(lake_gridded_eolis_data[lake_data.lake_id])} points returned, '\n",
        "          f'drawn from {len(lake_gridded_eolis_sources[lake_data.lake_id])} original sources.')"
      ],
      "metadata": {
        "id": "dujD9wdP_RPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the CryoTEMPO-EOLIS gridded data for the buffer zone around each subglacial lake"
      ],
      "metadata": {
        "id": "0oKddK89YCiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lakes_with_buffer_extent_df = lakes_extent_df.copy()\n",
        "# let's use the 'buffer' method to query data around each lake\n",
        "lakes_with_buffer_extent_df['geometry'] = lakes_extent_df.geometry.buffer(10000)\n",
        "lakes_with_buffer_extent_4326_df = lakes_with_buffer_extent_df.to_crs(epsg=4326)"
      ],
      "metadata": {
        "id": "ahrWcaFGYIyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_gridded_eolis_data = {}\n",
        "all_gridded_eolis_sources = {}\n",
        "\n",
        "buffer_gridded_eolis_data = {}\n",
        "\n",
        "for _, lake_data in lakes_with_buffer_extent_4326_df.iterrows():\n",
        "    query_start_time = perf_counter()\n",
        "    all_gridded_eolis_data[lake_data.lake_id], all_gridded_eolis_sources[lake_data.lake_id] = client.query_dataset(\n",
        "        dataset_id=gridded_product_dataset['dataset_id'],\n",
        "        epsg4326_polygon=lake_data.geometry,\n",
        "        min_datetime=datetime(2011, 1, 1),\n",
        "        max_datetime=datetime(2023, 9, 1),\n",
        "        columns_to_return=['timestamp', 'elevation', 'uncertainty'])\n",
        "\n",
        "    print(f'Query of subglacial lake with ID {lake_data.lake_id} complete in '\n",
        "          f'{perf_counter()-query_start_time:.2f} seconds, '\n",
        "          f'{len(all_gridded_eolis_data[lake_data.lake_id])} points returned, '\n",
        "          f'drawn from {len(all_gridded_eolis_sources[lake_data.lake_id])} original sources.')\n",
        "\n",
        "    # keep all the points that are not within the lake extent (i.e., that were not returned in the first query)\n",
        "    buffer_gridded_eolis_data[lake_data.lake_id] = pd.concat(\n",
        "        [all_gridded_eolis_data[lake_data.lake_id], lake_gridded_eolis_data[lake_data.lake_id]]).drop_duplicates(keep=False)\n",
        "\n",
        "    # perform just a sanity check: the number of points for the buffer zone should match the difference between the second query and the first\n",
        "    assert len(buffer_gridded_eolis_data[lake_data.lake_id]) == (len(all_gridded_eolis_data[lake_data.lake_id]) - len(lake_gridded_eolis_data[lake_data.lake_id]))\n",
        "    print(f'The buffer around lake with ID {lake_data.lake_id} '\n",
        "          f'has {len(buffer_gridded_eolis_data[lake_data.lake_id])} points, as expected.')"
      ],
      "metadata": {
        "id": "JN-e4WzrZFIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the elevation change timeseries\n",
        "\n",
        "We have created two complementary datasets with EOLIS gridded data, one over the lakes and one over the buffer area around the lakes.\n",
        "\n",
        "First calculate the elevation change separately for each dataset.\n",
        "Subsequently, derive the net elevation change of the subglacial lakes as the difference between the observed elevation change over the lakes and the average observed elevation change in the area surrounding the lakes."
      ],
      "metadata": {
        "id": "VtD_nV7_dCf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_time_series(gridded_product_data: gpd.GeoDataFrame) -> Tuple[NDArray, NDArray, List[int]]:\n",
        "    \"\"\"\n",
        "    Calculate a weighted average elevation change per month and associated errors.\n",
        "\n",
        "    Weights are assigned using the CryoTEMPO-EOLIS gridded product uncertainty.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    gridded_product_data : gpd.GeoDataFrame\n",
        "        CryoTEMPO-EOLIS gridded product data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[NDArray, NDArray, List[int]]\n",
        "        Weighted average elevation change elevation change per month, error on weighted average,\n",
        "        and dates of each timestep.\n",
        "    \"\"\"\n",
        "    unique_timestamps = sorted(gridded_product_data.timestamp.unique())\n",
        "\n",
        "    # elevation change is in reference to first gridded product\n",
        "    reference_dem = gridded_product_data[gridded_product_data.timestamp == unique_timestamps[0]]\n",
        "\n",
        "    # initialise empty lists for time series data\n",
        "    weighted_mean_timeseries = []\n",
        "    weighted_mean_errors = []\n",
        "    for unique_timestamp in unique_timestamps:\n",
        "        gridded_product_for_timestamp = gridded_product_data[gridded_product_data.timestamp == unique_timestamp]\n",
        "        merged_gdf = reference_dem.sjoin(df=gridded_product_for_timestamp, how='inner')\n",
        "        merged_gdf['elevation_difference'] = merged_gdf['elevation_right'] - merged_gdf['elevation_left']\n",
        "\n",
        "        # combine uncertainties for elevation difference\n",
        "        merged_gdf['elevation_difference_unc'] = np.sqrt(\n",
        "            merged_gdf['uncertainty_right']**2 + merged_gdf['uncertainty_left']**2)\n",
        "\n",
        "        # calculate average elevation change, weighted by measurement uncertainty\n",
        "        weighted_mean = (np.sum(merged_gdf['elevation_difference'] / merged_gdf['elevation_difference_unc']**2)\n",
        "                        / np.sum(1 / merged_gdf['elevation_difference_unc']**2))\n",
        "\n",
        "        # calculate weighted average uncertainty\n",
        "        error = np.sqrt(1 / np.sum(1 / merged_gdf['elevation_difference_unc']**2))\n",
        "\n",
        "        weighted_mean_timeseries.append(weighted_mean)\n",
        "        weighted_mean_errors.append(error)\n",
        "\n",
        "    dates = [datetime.fromtimestamp(ts) for ts in unique_timestamps]\n",
        "\n",
        "    return np.array(weighted_mean_timeseries), np.array(weighted_mean_errors), dates"
      ],
      "metadata": {
        "id": "ZVgjR3lU3Y_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_data = {}\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    buffer_mean_elevation_change, buffer_uncertainty_mean_elevation_change, buffer_timeseries_dates = calculate_time_series(buffer_gridded_eolis_data[lake_data.lake_id])\n",
        "    lake_mean_elevation_change, lake_uncertainty_mean_elevation_change, lake_timeseries_dates = calculate_time_series(lake_gridded_eolis_data[lake_data.lake_id])\n",
        "\n",
        "    # simple sanity check, to ensure that the observations are coincident over the lakes and over the surrounding areas\n",
        "    assert lake_timeseries_dates == buffer_timeseries_dates\n",
        "\n",
        "    timeseries_data[lake_data.lake_id] = pd.DataFrame.from_dict({\n",
        "        'date': lake_timeseries_dates,\n",
        "        'buffer_mean_elevation_change': buffer_mean_elevation_change,\n",
        "        'buffer_uncertainty_mean_elevation_change': buffer_uncertainty_mean_elevation_change,\n",
        "        'lake_mean_elevation_change': lake_mean_elevation_change,\n",
        "        'lake_uncertainty_mean_elevation_change': lake_uncertainty_mean_elevation_change})\n",
        "    # calculate the net elevation difference of the subglacial lakes and propagate the uncertainty\n",
        "    timeseries_data[lake_data.lake_id]['net_mean_elevation_change'] = lake_mean_elevation_change - buffer_mean_elevation_change\n",
        "    timeseries_data[lake_data.lake_id]['net_uncertainty_mean_elevation_change'] = np.sqrt(lake_uncertainty_mean_elevation_change**2 + buffer_uncertainty_mean_elevation_change**2)"
      ],
      "metadata": {
        "id": "Z9csb_zRdIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    timeseries_df = timeseries_data[lake_data.lake_id]\n",
        "    ax.plot(timeseries_df.date, timeseries_df.buffer_mean_elevation_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(timeseries_df.date,\n",
        "                    timeseries_df.buffer_mean_elevation_change - timeseries_df.buffer_uncertainty_mean_elevation_change,\n",
        "                    timeseries_df.buffer_mean_elevation_change + timeseries_df.buffer_uncertainty_mean_elevation_change,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Cumulative Elevation Change in buffer  [m]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Su-CgHbPeQZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    timeseries_df = timeseries_data[lake_data.lake_id]\n",
        "    ax.plot(timeseries_df.date, timeseries_df.net_mean_elevation_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(timeseries_df.date,\n",
        "                    timeseries_df.net_mean_elevation_change - timeseries_df.net_uncertainty_mean_elevation_change,\n",
        "                    timeseries_df.net_mean_elevation_change + timeseries_df.net_uncertainty_mean_elevation_change,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Net cumulative Elevation Change over lakes  [m]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CsSY-TrYfszc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for _, lake_data in lakes_extent_df.iterrows():\n",
        "\n",
        "    timeseries_df = timeseries_data[lake_data.lake_id]\n",
        "    volume_change = (timeseries_df.net_mean_elevation_change * lake_data.geometry.area) * 1e-9\n",
        "    uncertainty_volume_change = (timeseries_df.net_uncertainty_mean_elevation_change * lake_data.geometry.area) * 1e-9\n",
        "\n",
        "    # plot time series\n",
        "    ax.plot(timeseries_df.date, volume_change, lw=1, label=lake_data.lake_id)\n",
        "    ax.fill_between(timeseries_df.date,\n",
        "                    volume_change - uncertainty_volume_change,\n",
        "                    volume_change + uncertainty_volume_change,\n",
        "                    alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel(r'Cumulative Volume Change [km$^3$]')\n",
        "ax.legend(title='Subglacial lake ID')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1mXrDklzOU-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "elevation_change_as_morris_paper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}