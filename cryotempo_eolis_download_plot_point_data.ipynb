{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cryotempo_logo.png\" alt=\"logo\" width=\"200\"/> <img src=\"esa_logo.png\" alt=\"esa\" width=\"170\"/> <img src=\"earthwave_logo.png\" alt=\"earthwave\" width=\"150\"/> <img src=\"UoE_logo.png\" alt=\"uoe\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##<strong>*This is a Jupyter notebook that demonstrates how to download and use Cryotempo-EOLIS data, downloaded from cs2eo.org. Here, we will investigate the coverage and quality of the point product*</strong> \n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To run this notebook, you will need to make sure that the folllowing packages are installed in your python environment (all can be installed via pip/conda). If you are using the Google Colab environment, these packages are already installed.\n",
    "\n",
    "    - matplotlib\n",
    "    - pandas: for dataframe manipulation\n",
    "    - netCDF4: for handing NetCDF files\n",
    "    - glob\n",
    "    - datetime: for handling timestamps\n",
    "    - dateutil: for timseries generation\n",
    "    - numpy\n",
    "\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<strong>1) Download Data\n",
    "\n",
    "Regardless of whether you are using the Google Colab environment, or have downloaded this notebook to your local drive, you will first need to download some data. You can use this notebook to plot any CryoTEMPO-EOLIS point data that you choose. For a quick example, follow the below instructions to download a small example dataset.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary to run the rest of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdate\n",
    "import os\n",
    "from ftplib import FTP\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The following four cells contain the python script necessary to download the results of the query described above from the ESA FTP servers. It is taken verbatim from the script generated by a cs2eo query - one could run said script in place of these 4 cells, for any query of your choice. The data is downloaded to your current working directory. This query will a file containing point product data for and area of Greenland that includes the Jakobshavn glacier.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is necessary to complete the data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_byte_handler(data):\n",
    "    global download_file_obj, read_byte_count, total_byte_count\n",
    "    download_file_obj.write(data)\n",
    "    read_byte_count = read_byte_count + len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we list the files that we want to download. These are generated automatically by a [cs2eo.org](https://cs2eo.org/cryotempo) query. If you wanted to download additional months or a different region, you would add the corresponding file paths to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_files =  ['/TEMPO_SWATH_POINT/2023/01/GREENLAND/CS_OFFL_THEM_POINT_GREENLAND_2023_01_-200000___-2300000__V202.nc',\n",
    "              '/TEMPO_SWATH_POINT/2023/02/GREENLAND/CS_OFFL_THEM_POINT_GREENLAND_2023_02_-200000___-2300000__V202.nc']\n",
    "download_file_obj = None\n",
    "read_byte_count = None\n",
    "total_byte_count = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must provide an email address to access the ESA FTP server. Input your email into the prompt that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = input(\"Please enter your e-mail: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we connect to the server, supply our email address and download the data to our current working directory. This process should take a couple of minutes. The progress of the download will be shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"About to connect to ESA science server\")\n",
    "# Open the FTP connection\n",
    "with FTP(\"science-pds.cryosat.esa.int\") as ftp:\n",
    "    try:\n",
    "        #Login using your email address\n",
    "        ftp.login(\"anonymous\", email)\n",
    "        print(\"Downloading {} files\".format(len(esa_files)))\n",
    "        # Step through the files that we want to download    \n",
    "        for i, filename in enumerate(esa_files):\n",
    "            padded_count = str(i+1).zfill(len(str(len(esa_files))))\n",
    "            # Print some details about the progress of the download\n",
    "            print(\"{}/{}. Downloading file {}\".format(padded_count, len(esa_files), os.path.basename(filename)))\n",
    "            with open(os.path.basename(filename), 'wb') as download_file:\n",
    "                download_file_obj = download_file\n",
    "                total_byte_count = ftp.size(filename)\n",
    "                read_byte_count = 0\n",
    "                ftp.retrbinary('RETR ' + filename, file_byte_handler, 1024)\n",
    "            print(\"\\n\")\n",
    "    # After downloading all of the files, close the FTP connection\n",
    "    finally:\n",
    "        print(\"Exiting FTP.\")\n",
    "        ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "###<strong>2) Load downloaded data into python</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Now that we have downloaded the data, we can use python to visualise it. In the remainder of this demo notebook, we will load the data that we have just downloaded into python, and use it to investigate the coverage and quality of the CryoTEMPO-EOLIS point prodcut over the Jakobshavn glacier.</strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have downloaded the notebook onto your local machine, you will need to edit the filepath below to point to the location of the downloaded data. We will then locate and store the names of each of the point data files. The point data is stored in *netcdf* files, which have the suffix '.nc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = './content/' #Filepath specific to the Google Colab environment\n",
    "files = sorted(glob.glob(data_filepath+'/*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Now that the files containing the data have been located, we will load the data that they contain into python, one file at a time.</strong> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes_from_netcdfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we can check the metadata for the point product files. In the output of this command, we can see lots of additional information assosciated with the point product files, including projection information, product version and support information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nc.Dataset(files[0])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code opens each netcdf file, reads in the data associated with each variable, and stores this data in the column of a dataframe. The loop below does this for all of the point data files in our current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    data = {}\n",
    "    with nc.Dataset(files[i]) as netcdf:\n",
    "        for column in netcdf.variables:\n",
    "            rows_from_nc = netcdf.variables[column]\n",
    "            data[column] = pd.Series(rows_from_nc[:])\n",
    "    df = pd.DataFrame(data)        \n",
    "    list_of_dataframes_from_netcdfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in the data from each individual file returned by the point product query, we combine them into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_query_results = pd.concat(list_of_dataframes_from_netcdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "###<strong>3. Investigate the point product</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Now that the data has been loaded into the notebook, and stored in a dataframe, we can create some visuals that allow us to quickly understand the scope of the point product and its capabilities.</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the cell below, we can see the total number of points in the data. Additionally, the variables available for analysis include a time, x and y position, elevation and an associated uncertainty value. For more information about the derivation of these variables, see the CryoTEMPO-EOLIS ATBD, available at https://cryotempo-eolis.org/product-description/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe meta data that describes results of query\n",
    "print('Number of points in query =', len(full_query_results))\n",
    "print('Variables returned by query: ', full_query_results.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a simple spatial plot of the point product that we have downloaded. This plot shows all point data available for the Jakobshavn glacier in the timeframe that we specified for the query, and demonstrates the spatial coverage achieved using swath processing. In the Figure on the left, the scatter points are colour-coded by their elevations. On the right, the colour demonstrates the uncertainty associated with each point. For more information on the observing tracks of CryoSat and swath processing, see https://earth.esa.int/eogateway/news/cryosats-swath-processing-technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(30,15), sharex=True)\n",
    "\n",
    "c = axs[0].scatter(full_query_results['x'], full_query_results['y'], c=full_query_results['elevation'], s=1, cmap='viridis')\n",
    "plt.colorbar(c, ax = axs[0], label='Elevation [m]')\n",
    "u = axs[1].scatter(full_query_results['x'], full_query_results['y'], c=full_query_results['uncertainty'], s=1, cmap='viridis')\n",
    "plt.colorbar(u, ax = axs[1], label='Uncertainty [m]')\n",
    "\n",
    "axs[0].set_xlabel('x [m]')\n",
    "axs[1].set_xlabel('x [m]')\n",
    "axs[0].set_ylabel('y [m]')\n",
    "plt.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "t = plt.suptitle('Coverage of CryoTEMPO-EOLIS point product over the Jakobshavn glacier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Next, we can calculate some data quality statistics. </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we calculate the mean and median uncertainties, as well as the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_unc = np.nanmean(full_query_results['uncertainty'])\n",
    "med_unc = np.nanmedian(full_query_results['uncertainty'])\n",
    "std_unc = np.std(full_query_results['uncertainty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next set up some histogram bins, requiring 40 bins in the uncertainty range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(np.nanmin(full_query_results['uncertainty']),np.nanmax(full_query_results['uncertainty']),40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot a histogram of the uncertainty values for this point product dataset, and display the mean and median values, as well as the standard deviation of uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(20,10))\n",
    "\n",
    "a = axs.hist(full_query_results['uncertainty'], bins = bins, facecolor='paleturquoise', edgecolor='k', alpha=0.6)\n",
    "\n",
    "axs.vlines(mean_unc, 0, 2.5e4, color='k', linestyle='dashed', label='Mean Uncertainty = {:0.2f}'.format(mean_unc))\n",
    "axs.vlines(med_unc, 0, 2.5e4, color='b', linestyle='dashed', label='Median Uncertainty = {:0.2f}'.format(med_unc))\n",
    "axs.vlines(mean_unc - std_unc, 0, 2.5e4, color='orange', linestyle='dashed')\n",
    "axs.vlines(mean_unc + std_unc, 0, 2.5e4, color='orange', linestyle='dashed',label='$\\sigma$ = {:0.2f}'.format(std_unc))\n",
    "\n",
    "axs.set_ylim(0, 2.5e4)\n",
    "axs.set_xlabel('Uncertainty [m]')\n",
    "axs.set_ylabel('Frequency')\n",
    "axs.set_title('Uncertainty distribution for CryoTEMPO-EOLIS point product')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ew_eolis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2acbfda774a9eee78609fae0795920b9a29a7265f1d42eeff10c43fa0d5c3170"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
